{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdd6e30-82bb-4a74-a7f8-e712dc07e80e",
   "metadata": {},
   "source": [
    "# This notebook goes through process of training Random Forest model and displays model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c89e7b-c9cd-426a-a9d5-6264818884c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6fd51b-6c4c-4483-9c51-2e1459371b73",
   "metadata": {},
   "source": [
    "# Unit test to determine if file has all data necessary to run Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42d98ba-a9f3-4721-857d-5a8ba563c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_test(df):\n",
    "    '''Test to ensure all environmental data is in csv file and none is missing'''\n",
    "    if df.isnull().sum().all() == 0:\n",
    "        print('✅ Data is correctly read in and ready for training!')\n",
    "    else:\n",
    "        sys.exit('❌ Error. One or more values are missing. Please check csv file to remove any dates with incomplete environmental values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebcdc1-415f-42c1-9955-8935cfc35ccf",
   "metadata": {},
   "source": [
    "# Read in environmental and MD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3081c47-4175-4f53-8a0d-a49c92787938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data is correctly read in and ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Read in dataframe\n",
    "df = pd.read_csv(\"central_ok_mds_env_FINAL.csv\")\n",
    "\n",
    "# Complete unit test\n",
    "csv_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484deea8-877c-450d-8471-9cd3bc4c2d62",
   "metadata": {},
   "source": [
    "# Split data into training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b22717b-5c83-4237-9d2c-e8206809aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data, val_test_data = train_test_split(df, test_size = 0.3, random_state = 988)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size = 0.6667, random_state = 988)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca42b8b-3b2d-46cc-8340-a42e82c27690",
   "metadata": {},
   "source": [
    "# Train base Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54cde6cd-6cbd-4ced-b41a-27600dc17dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set feature list of MD and RAP analysis data\n",
    "feature_list = ['month', 'time', 'pwat', 'mslp', 'cape', 'cin', 't2m', 'rh2m', 'q2m', 'u10', 'v10', 'uv10', 'lcl', 'shr0_6', 'srh0_1', 'srh0_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2d87df-efbf-43e2-923d-2294049f41c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(random_state=56):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75        48\n",
      "           1       0.77      0.69      0.73        49\n",
      "\n",
      "    accuracy                           0.74        97\n",
      "   macro avg       0.74      0.74      0.74        97\n",
      "weighted avg       0.75      0.74      0.74        97\n",
      "\n",
      "\n",
      "Brier score: 0.25773195876288657\n",
      "\n",
      "1. (time) feature 1 (0.155708)\n",
      "2. (pwat) feature 2 (0.073518)\n",
      "3. (srh0_3) feature 15 (0.068933)\n",
      "4. (shr0_6) feature 13 (0.066249)\n",
      "5. (cin) feature 5 (0.062023)\n",
      "6. (mslp) feature 3 (0.059004)\n",
      "7. (srh0_1) feature 14 (0.058645)\n",
      "8. (lcl) feature 12 (0.057585)\n",
      "9. (rh2m) feature 7 (0.055991)\n",
      "10. (t2m) feature 6 (0.055883)\n",
      "11. (q2m) feature 8 (0.054606)\n",
      "12. (u10) feature 9 (0.054566)\n",
      "13. (uv10) feature 11 (0.051042)\n",
      "14. (cape) feature 4 (0.049007)\n",
      "15. (v10) feature 10 (0.048725)\n",
      "16. (month) feature 0 (0.028514)\n"
     ]
    }
   ],
   "source": [
    "# Train base Random Forest model\n",
    "rf = RandomForestClassifier(random_state = 56)\n",
    "rf.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test (using validation data) base Random Forest model\n",
    "predicted = rf.predict(val_data[feature_list].values)\n",
    "expected = val_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print('Classification report for classifier %s:\\n%s\\n'\n",
    "      % (rf, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(val_data[feature_list].values.shape[1]):\n",
    "    print('%d. (%s) feature %d (%f)' % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "    \n",
    "# Save Random Forest model as joblib file    \n",
    "# joblib.dump(rf, 'random_forest_base.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e57225-3458-4130-8860-ceef10890f26",
   "metadata": {},
   "source": [
    "# Optimized Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65fbbfdb-f2f7-442d-aa27-2c1bd67bea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pyEAE/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "4704 fits failed out of a total of 47040.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4704 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 476, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 189, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 265, in fit\n",
      "    check_scalar(\n",
      "  File \"/anaconda3/envs/pyEAE/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split == 1, must be >= 2.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/anaconda3/envs/pyEAE/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.71364499 0.71509401 0.71509401]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 4,\n",
       " 'n_estimators': 150}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create parameterizations to test on training data for optimization\n",
    "param_grid = [{\n",
    "    'n_estimators': [20, 40, 60, 80, 100, 150, 200],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'min_samples_split': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20] }]\n",
    "\n",
    "# Fit optizimation to training data\n",
    "grid_search = GridSearchCV(rf, param_grid, cv = 2, scoring = 'accuracy', n_jobs = -1)\n",
    "\n",
    "grid_search.fit(train_data[feature_list].values, train_data.label.values)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761c34e5-063b-48d8-994f-4cd8908fed31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=700):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78        48\n",
      "           1       0.79      0.76      0.77        49\n",
      "\n",
      "    accuracy                           0.77        97\n",
      "   macro avg       0.77      0.77      0.77        97\n",
      "weighted avg       0.77      0.77      0.77        97\n",
      "\n",
      "\n",
      "Brier score: 0.2268041237113402\n",
      "\n",
      "1. (time) feature 1 (0.214611)\n",
      "2. (shr0_6) feature 13 (0.075268)\n",
      "3. (pwat) feature 2 (0.072107)\n",
      "4. (srh0_3) feature 15 (0.070033)\n",
      "5. (cin) feature 5 (0.067436)\n",
      "6. (mslp) feature 3 (0.054553)\n",
      "7. (rh2m) feature 7 (0.051457)\n",
      "8. (u10) feature 9 (0.048996)\n",
      "9. (srh0_1) feature 14 (0.048635)\n",
      "10. (q2m) feature 8 (0.046989)\n",
      "11. (lcl) feature 12 (0.046830)\n",
      "12. (t2m) feature 6 (0.045960)\n",
      "13. (v10) feature 10 (0.045799)\n",
      "14. (cape) feature 4 (0.045410)\n",
      "15. (uv10) feature 11 (0.043785)\n",
      "16. (month) feature 0 (0.022130)\n"
     ]
    }
   ],
   "source": [
    "# Train optimized Random Forest model\n",
    "rf_opt = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 700)\n",
    "rf_opt.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test (using validation data) optimized Random Forest model\n",
    "predicted = rf_opt.predict(val_data[feature_list].values)\n",
    "expected = val_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_opt, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf_opt.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(val_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "    \n",
    "# Save Random Forest model as joblib file\n",
    "# joblib.dump(rf_opt, \"random_forest_opt.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd173de6-61df-4681-a891-1b86ea654f38",
   "metadata": {},
   "source": [
    "# Testing and perturbing opitmized Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec7d641-b963-4f13-b0d8-e161255bf4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=136):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.69      0.72       105\n",
      "           1       0.68      0.76      0.72        91\n",
      "\n",
      "    accuracy                           0.72       196\n",
      "   macro avg       0.72      0.72      0.72       196\n",
      "weighted avg       0.72      0.72      0.72       196\n",
      "\n",
      "\n",
      "Brier score: 0.28061224489795916\n",
      "\n",
      "1. (time) feature 1 (0.195804)\n",
      "2. (srh0_3) feature 15 (0.082093)\n",
      "3. (pwat) feature 2 (0.077716)\n",
      "4. (shr0_6) feature 13 (0.074642)\n",
      "5. (cin) feature 5 (0.063688)\n",
      "6. (mslp) feature 3 (0.056502)\n",
      "7. (rh2m) feature 7 (0.055640)\n",
      "8. (lcl) feature 12 (0.051827)\n",
      "9. (v10) feature 10 (0.051563)\n",
      "10. (cape) feature 4 (0.050679)\n",
      "11. (u10) feature 9 (0.047859)\n",
      "12. (t2m) feature 6 (0.045712)\n",
      "13. (srh0_1) feature 14 (0.043964)\n",
      "14. (q2m) feature 8 (0.043452)\n",
      "15. (uv10) feature 11 (0.041251)\n",
      "16. (month) feature 0 (0.017609)\n"
     ]
    }
   ],
   "source": [
    "# Train perturbed final Random Forest model\n",
    "rf_final0 = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 136)\n",
    "rf_final0.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test perturbed final Random Forest model\n",
    "predicted = rf_final0.predict(test_data[feature_list].values)\n",
    "expected = test_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_final0, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "importances = rf_final0.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(test_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "\n",
    "# Save Random Forest model as joblib file\n",
    "# joblib.dump(rf_final0, \"random_forest_final0.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a23c0153-ce9a-4989-a447-6eb3dc240722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=2):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.65      0.68       105\n",
      "           1       0.63      0.69      0.66        91\n",
      "\n",
      "    accuracy                           0.67       196\n",
      "   macro avg       0.67      0.67      0.67       196\n",
      "weighted avg       0.67      0.67      0.67       196\n",
      "\n",
      "\n",
      "Brier score: 0.33163265306122447\n",
      "\n",
      "1. (time) feature 1 (0.205736)\n",
      "2. (pwat) feature 2 (0.081016)\n",
      "3. (srh0_3) feature 15 (0.078090)\n",
      "4. (shr0_6) feature 13 (0.067934)\n",
      "5. (srh0_1) feature 14 (0.066911)\n",
      "6. (q2m) feature 8 (0.055840)\n",
      "7. (rh2m) feature 7 (0.054702)\n",
      "8. (mslp) feature 3 (0.054327)\n",
      "9. (lcl) feature 12 (0.049814)\n",
      "10. (u10) feature 9 (0.048007)\n",
      "11. (cape) feature 4 (0.047483)\n",
      "12. (t2m) feature 6 (0.044772)\n",
      "13. (cin) feature 5 (0.044702)\n",
      "14. (v10) feature 10 (0.042641)\n",
      "15. (uv10) feature 11 (0.034995)\n",
      "16. (month) feature 0 (0.023028)\n"
     ]
    }
   ],
   "source": [
    "# Train perturbed final Random Forest model\n",
    "rf_final1 = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 2)\n",
    "rf_final1.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test perturbed final Random Forest model\n",
    "predicted = rf_final1.predict(test_data[feature_list].values)\n",
    "expected = test_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_final1, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf_final1.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(test_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "\n",
    "# Save Random Forest model as joblib file\n",
    "# joblib.dump(rf_final1, \"random_forest_final1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d39326a-ca1b-4449-a96a-8cb638d58884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=439):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.67      0.71       105\n",
      "           1       0.66      0.75      0.70        91\n",
      "\n",
      "    accuracy                           0.70       196\n",
      "   macro avg       0.71      0.71      0.70       196\n",
      "weighted avg       0.71      0.70      0.70       196\n",
      "\n",
      "\n",
      "Brier score: 0.29591836734693877\n",
      "\n",
      "1. (time) feature 1 (0.185100)\n",
      "2. (shr0_6) feature 13 (0.078559)\n",
      "3. (srh0_3) feature 15 (0.078368)\n",
      "4. (pwat) feature 2 (0.069783)\n",
      "5. (cin) feature 5 (0.066507)\n",
      "6. (rh2m) feature 7 (0.061962)\n",
      "7. (srh0_1) feature 14 (0.058714)\n",
      "8. (mslp) feature 3 (0.054137)\n",
      "9. (t2m) feature 6 (0.052497)\n",
      "10. (q2m) feature 8 (0.051303)\n",
      "11. (v10) feature 10 (0.050193)\n",
      "12. (u10) feature 9 (0.046541)\n",
      "13. (uv10) feature 11 (0.045853)\n",
      "14. (lcl) feature 12 (0.041994)\n",
      "15. (cape) feature 4 (0.038584)\n",
      "16. (month) feature 0 (0.019904)\n"
     ]
    }
   ],
   "source": [
    "# Train perturbed final Random Forest model\n",
    "rf_final2 = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 439)\n",
    "rf_final2.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test perturbed final Random Forest model\n",
    "predicted = rf_final2.predict(test_data[feature_list].values)\n",
    "expected = test_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_final2, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf_final2.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(test_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "    \n",
    "# Save Random Forest model as joblib file    \n",
    "# joblib.dump(rf_final2, \"random_forest_final2.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b562a84c-080d-4354-8b57-a2a6e8823d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=712):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.68       105\n",
      "           1       0.64      0.69      0.66        91\n",
      "\n",
      "    accuracy                           0.67       196\n",
      "   macro avg       0.67      0.67      0.67       196\n",
      "weighted avg       0.68      0.67      0.67       196\n",
      "\n",
      "\n",
      "Brier score: 0.32653061224489793\n",
      "\n",
      "1. (time) feature 1 (0.177671)\n",
      "2. (shr0_6) feature 13 (0.076150)\n",
      "3. (pwat) feature 2 (0.073795)\n",
      "4. (srh0_3) feature 15 (0.070539)\n",
      "5. (cin) feature 5 (0.064363)\n",
      "6. (srh0_1) feature 14 (0.062376)\n",
      "7. (mslp) feature 3 (0.058916)\n",
      "8. (rh2m) feature 7 (0.057859)\n",
      "9. (lcl) feature 12 (0.055567)\n",
      "10. (q2m) feature 8 (0.053701)\n",
      "11. (cape) feature 4 (0.049548)\n",
      "12. (v10) feature 10 (0.046250)\n",
      "13. (uv10) feature 11 (0.045149)\n",
      "14. (u10) feature 9 (0.044993)\n",
      "15. (t2m) feature 6 (0.043733)\n",
      "16. (month) feature 0 (0.019389)\n"
     ]
    }
   ],
   "source": [
    "# Train perturbed final Random Forest model\n",
    "rf_final3 = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 712)\n",
    "rf_final3.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test perturbed final Random Forest model\n",
    "predicted = rf_final3.predict(test_data[feature_list].values)\n",
    "expected = test_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_final3, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf_final3.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(test_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "    \n",
    "# Save Random Forest model as joblib file    \n",
    "# joblib.dump(rf_final3, \"random_forest_final3.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1053967-6bcf-4670-9809-114dfe971f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=297):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.67      0.71       105\n",
      "           1       0.66      0.76      0.71        91\n",
      "\n",
      "    accuracy                           0.71       196\n",
      "   macro avg       0.71      0.71      0.71       196\n",
      "weighted avg       0.72      0.71      0.71       196\n",
      "\n",
      "\n",
      "Brier score: 0.29081632653061223\n",
      "\n",
      "1. (time) feature 1 (0.190131)\n",
      "2. (srh0_3) feature 15 (0.096217)\n",
      "3. (pwat) feature 2 (0.076019)\n",
      "4. (shr0_6) feature 13 (0.068551)\n",
      "5. (q2m) feature 8 (0.059210)\n",
      "6. (rh2m) feature 7 (0.057895)\n",
      "7. (cin) feature 5 (0.052489)\n",
      "8. (srh0_1) feature 14 (0.052375)\n",
      "9. (u10) feature 9 (0.052164)\n",
      "10. (cape) feature 4 (0.051808)\n",
      "11. (t2m) feature 6 (0.047685)\n",
      "12. (lcl) feature 12 (0.045393)\n",
      "13. (v10) feature 10 (0.044606)\n",
      "14. (mslp) feature 3 (0.041311)\n",
      "15. (uv10) feature 11 (0.037235)\n",
      "16. (month) feature 0 (0.026909)\n"
     ]
    }
   ],
   "source": [
    "# Train perturbed final Random Forest model\n",
    "rf_final4 = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 297)\n",
    "rf_final4.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test perturbed final Random Forest model\n",
    "predicted = rf_final4.predict(test_data[feature_list].values)\n",
    "expected = test_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_final4, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf_final4.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(test_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "    \n",
    "# Save Random Forest model as joblib file    \n",
    "# joblib.dump(rf_final4, \"random_forest_final4.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "676d9137-8acf-41be-b709-be70f6b7349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=331):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.65      0.68       105\n",
      "           1       0.64      0.71      0.67        91\n",
      "\n",
      "    accuracy                           0.68       196\n",
      "   macro avg       0.68      0.68      0.68       196\n",
      "weighted avg       0.68      0.68      0.68       196\n",
      "\n",
      "\n",
      "Brier score: 0.32142857142857145\n",
      "\n",
      "1. (time) feature 1 (0.183461)\n",
      "2. (srh0_3) feature 15 (0.082607)\n",
      "3. (pwat) feature 2 (0.074283)\n",
      "4. (rh2m) feature 7 (0.063791)\n",
      "5. (cin) feature 5 (0.060996)\n",
      "6. (cape) feature 4 (0.057963)\n",
      "7. (srh0_1) feature 14 (0.056448)\n",
      "8. (shr0_6) feature 13 (0.054206)\n",
      "9. (mslp) feature 3 (0.052167)\n",
      "10. (u10) feature 9 (0.051692)\n",
      "11. (q2m) feature 8 (0.050714)\n",
      "12. (lcl) feature 12 (0.049677)\n",
      "13. (uv10) feature 11 (0.048357)\n",
      "14. (v10) feature 10 (0.047448)\n",
      "15. (t2m) feature 6 (0.044304)\n",
      "16. (month) feature 0 (0.021887)\n"
     ]
    }
   ],
   "source": [
    "# Train perturbed final Random Forest model\n",
    "rf_final5 = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 331)\n",
    "rf_final5.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test perturbed final Random Forest model\n",
    "predicted = rf_final5.predict(test_data[feature_list].values)\n",
    "expected = test_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_final5, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf_final5.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(test_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "    \n",
    "# Save Random Forest model as joblib file    \n",
    "# joblib.dump(rf_final5, \"random_forest_final5.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7610bab0-8b66-4abe-a78c-3136736dc791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=984):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71       105\n",
      "           1       0.66      0.67      0.67        91\n",
      "\n",
      "    accuracy                           0.69       196\n",
      "   macro avg       0.69      0.69      0.69       196\n",
      "weighted avg       0.69      0.69      0.69       196\n",
      "\n",
      "\n",
      "Brier score: 0.3112244897959184\n",
      "\n",
      "1. (time) feature 1 (0.187416)\n",
      "2. (srh0_3) feature 15 (0.076926)\n",
      "3. (pwat) feature 2 (0.076831)\n",
      "4. (shr0_6) feature 13 (0.062339)\n",
      "5. (t2m) feature 6 (0.061379)\n",
      "6. (cin) feature 5 (0.057303)\n",
      "7. (mslp) feature 3 (0.054607)\n",
      "8. (lcl) feature 12 (0.054082)\n",
      "9. (q2m) feature 8 (0.053586)\n",
      "10. (cape) feature 4 (0.052116)\n",
      "11. (rh2m) feature 7 (0.050641)\n",
      "12. (u10) feature 9 (0.049978)\n",
      "13. (srh0_1) feature 14 (0.048374)\n",
      "14. (v10) feature 10 (0.046912)\n",
      "15. (uv10) feature 11 (0.039072)\n",
      "16. (month) feature 0 (0.028438)\n"
     ]
    }
   ],
   "source": [
    "# Train perturbed final Random Forest model\n",
    "rf_final6 = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 984)\n",
    "rf_final6.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test perturbed final Random Forest model\n",
    "predicted = rf_final6.predict(test_data[feature_list].values)\n",
    "expected = test_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_final6, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf_final6.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(test_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "    \n",
    "# Save Random Forest model as joblib file    \n",
    "# joblib.dump(rf_final6, \"random_forest_final6.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0963648-c3db-410b-9174-69970fc5f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=813):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.66      0.69       105\n",
      "           1       0.64      0.71      0.68        91\n",
      "\n",
      "    accuracy                           0.68       196\n",
      "   macro avg       0.68      0.69      0.68       196\n",
      "weighted avg       0.69      0.68      0.68       196\n",
      "\n",
      "\n",
      "Brier score: 0.3163265306122449\n",
      "\n",
      "1. (time) feature 1 (0.179408)\n",
      "2. (srh0_3) feature 15 (0.083320)\n",
      "3. (pwat) feature 2 (0.075837)\n",
      "4. (shr0_6) feature 13 (0.073692)\n",
      "5. (srh0_1) feature 14 (0.070514)\n",
      "6. (cin) feature 5 (0.060522)\n",
      "7. (q2m) feature 8 (0.053300)\n",
      "8. (u10) feature 9 (0.052594)\n",
      "9. (lcl) feature 12 (0.051187)\n",
      "10. (t2m) feature 6 (0.051182)\n",
      "11. (v10) feature 10 (0.047428)\n",
      "12. (uv10) feature 11 (0.047110)\n",
      "13. (mslp) feature 3 (0.046742)\n",
      "14. (rh2m) feature 7 (0.043757)\n",
      "15. (cape) feature 4 (0.041833)\n",
      "16. (month) feature 0 (0.021572)\n"
     ]
    }
   ],
   "source": [
    "# Train perturbed final Random Forest model\n",
    "rf_final7 = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 813)\n",
    "rf_final7.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test perturbed final Random Forest model\n",
    "predicted = rf_final7.predict(test_data[feature_list].values)\n",
    "expected = test_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_final7, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf_final7.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(test_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "    \n",
    "# Save Random Forest model as joblib file    \n",
    "# joblib.dump(rf_final7, \"random_forest_final7.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51b9e06a-448a-44da-8a9e-0ebe012cf89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=47):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.61      0.67       105\n",
      "           1       0.62      0.75      0.68        91\n",
      "\n",
      "    accuracy                           0.67       196\n",
      "   macro avg       0.68      0.68      0.67       196\n",
      "weighted avg       0.68      0.67      0.67       196\n",
      "\n",
      "\n",
      "Brier score: 0.32653061224489793\n",
      "\n",
      "1. (time) feature 1 (0.201208)\n",
      "2. (shr0_6) feature 13 (0.079029)\n",
      "3. (srh0_3) feature 15 (0.078900)\n",
      "4. (pwat) feature 2 (0.069183)\n",
      "5. (cin) feature 5 (0.063931)\n",
      "6. (lcl) feature 12 (0.062670)\n",
      "7. (srh0_1) feature 14 (0.055663)\n",
      "8. (rh2m) feature 7 (0.051547)\n",
      "9. (q2m) feature 8 (0.050994)\n",
      "10. (v10) feature 10 (0.045971)\n",
      "11. (t2m) feature 6 (0.045806)\n",
      "12. (mslp) feature 3 (0.044868)\n",
      "13. (cape) feature 4 (0.043263)\n",
      "14. (uv10) feature 11 (0.041625)\n",
      "15. (u10) feature 9 (0.040202)\n",
      "16. (month) feature 0 (0.025139)\n"
     ]
    }
   ],
   "source": [
    "# Train perturbed final Random Forest model\n",
    "rf_final8 = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 47)\n",
    "rf_final8.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test perturbed final Random Forest model\n",
    "predicted = rf_final8.predict(test_data[feature_list].values)\n",
    "expected = test_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_final8, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf_final8.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(test_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "    \n",
    "# Save Random Forest model as joblib file    \n",
    "# joblib.dump(rf_final8, \"random_forest_final8.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48104705-30a3-4b7d-b887-d37b5d856195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(criterion='log_loss', max_depth=16, min_samples_leaf=6,\n",
      "                       min_samples_split=4, n_estimators=40, random_state=454):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72       105\n",
      "           1       0.67      0.73      0.70        91\n",
      "\n",
      "    accuracy                           0.71       196\n",
      "   macro avg       0.71      0.71      0.71       196\n",
      "weighted avg       0.71      0.71      0.71       196\n",
      "\n",
      "\n",
      "Brier score: 0.29081632653061223\n",
      "\n",
      "1. (time) feature 1 (0.194181)\n",
      "2. (srh0_3) feature 15 (0.070899)\n",
      "3. (pwat) feature 2 (0.070563)\n",
      "4. (cin) feature 5 (0.065974)\n",
      "5. (shr0_6) feature 13 (0.064757)\n",
      "6. (rh2m) feature 7 (0.061923)\n",
      "7. (lcl) feature 12 (0.059431)\n",
      "8. (v10) feature 10 (0.058531)\n",
      "9. (mslp) feature 3 (0.055844)\n",
      "10. (srh0_1) feature 14 (0.049973)\n",
      "11. (cape) feature 4 (0.048866)\n",
      "12. (t2m) feature 6 (0.045192)\n",
      "13. (u10) feature 9 (0.045189)\n",
      "14. (uv10) feature 11 (0.044079)\n",
      "15. (q2m) feature 8 (0.040273)\n",
      "16. (month) feature 0 (0.024325)\n"
     ]
    }
   ],
   "source": [
    "# Train perturbed final Random Forest model\n",
    "rf_final9 = RandomForestClassifier(criterion = 'log_loss', max_depth = 16, min_samples_leaf = 6, min_samples_split = 4, n_estimators = 40, random_state = 454)\n",
    "rf_final9.fit(train_data[feature_list].values, train_data.label.values)\n",
    "\n",
    "# Test perturbed final Random Forest model\n",
    "predicted = rf_final9.predict(test_data[feature_list].values)\n",
    "expected = test_data.label.values\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rf_final9, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "# Print brier score\n",
    "print(f'Brier score: {metrics.brier_score_loss(expected, predicted)}', end = '\\n\\n')\n",
    "\n",
    "# Print feature importance precentages\n",
    "importances = rf_final9.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(test_data[feature_list].values.shape[1]):\n",
    "    print(\"%d. (%s) feature %d (%f)\" % (i+1, feature_list[indices[i]], indices[i], importances[indices[i]]))\n",
    "    \n",
    "# Save Random Forest model as joblib file    \n",
    "# joblib.dump(rf_final9, \"random_forest_final9.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyEAE]",
   "language": "python",
   "name": "conda-env-pyEAE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
